# word_discovery
## 数据：

36Kr新闻5000条数据

## 方法1：

【中文分词系列】 8. 更好的新词发现算法kexue.fm

苏神基于Ngrams凝固度的算法，通过凝固度获取凝固的片段，通过凝固片段的自由扩展获得词的边界，而不需要另外通过边界熵来确定边界，使得长度可以任意长[【中文分词系列】 8. 更好的新词发现算法](https://kexue.fm/archives/4256/comment-page-3#comments)苏神基于Ngrams凝固度的算法，通过凝固度获取凝固的片段，通过凝固片段的自由扩展获得词的边界，而不需要另外通过边界熵来确定边界，使得长度可以任意长

算法：

1. 统计：统计ngrams频率，计算内部凝固度，删除小于一定阈值的词（不同ngrams设置不同的阈值）
2. 切分：用上述grams对语料进行切分（粗糙的分词），并统计频率。切分的规则是，只要一个片段出现在前一步得到的集合G中，这个片段就不切分，比如“各项目”，只要“各项”和“项目”都在G中，这时候就算“各项目”不在G中，那么“各项目”还是不切分，保留下来；
3. 回溯：检查如果它是一个小于等于n字的词，那么检测它在不在G中，不在就出局；如果它是一个大于n字的词，那个检测它每个n字片段是不是在G中，只要有一个片段不在，就出局。

具体：

ngrams取4， 最小词频5，凝聚度筛选{2: 5, 3: 25, 4: 125}

## 方法2：

smoothnlp

https://github.com/smoothnlp/SmoothNLPgithub.com

利用内部凝聚度和左右信息熵计算得分

算法：

1. 生成候选词：直接将文本按字符分割后拼接为候选词。即把文本按标点切分成句，提取每句话的ngrams
2. 候选词得分计算：计算每个候选词得分，表示成词可能性，度量左右邻字丰富程度使用了左右信息熵差的绝对值构造的统计量L，度量内聚程度使用的平均互信息AMI，得分=L+AMI

具体：

min_n=2, max_n=4，因此最大的词只能是4，最小词频5

## 结果对比：

使用jieba分词对语料进行分词得到词表，将上述两种算法所得词表与jieba分词作比较，看看有哪些词是jieba分词结果没有的，是否为’新词‘

smoothnlp是内部凝聚度加左右信息熵得分排序，苏神的算法也按凝聚度做个排序，结果如下

![img](https://pic1.zhimg.com/80/v2-6470175f5c9535cc66501dc1fa4933d5_720w.png)

可以看到smoothnlp top50有36氪、机器学习、新浪微博、比特币等，但这些可能是jieba分成了机器、学习，新浪、微博之类的

苏神的算法top50有很多的人名，还有钢铁侠、蚂蜂窝、蜗牛邦、石墨烯、树莓派、抓钱猫、捞月狗、纸牌屋、迷你仓等新兴词汇

再比较下两者互相没有的词：

![img](https://pic4.zhimg.com/80/v2-ee92a7200ab29c8fa161019a9591bb24_720w.png)

上面是苏神算法里有而smoothnlp里没有的，下面反之

上面的抓钱猫、纸牌屋、迷你仓、蜜爸妈都是很新的词，还有海底捞、余额宝、KK直播、科大讯飞

## 结论

初次使用，苏神的算法结果明显好于smoothnlp（也可能我的打开方式不对），不过效率需要优化下