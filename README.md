# word_discovery
## 数据：

36Kr新闻5000条数据

## 方法1：

【中文分词系列】 8. 更好的新词发现算法kexue.fm

苏神基于Ngrams凝固度的算法，通过凝固度获取凝固的片段，通过凝固片段的自由扩展获得词的边界，而不需要另外通过边界熵来确定边界，使得长度可以任意长[【中文分词系列】 8. 更好的新词发现算法](https://kexue.fm/archives/4256/comment-page-3#comments)苏神基于Ngrams凝固度的算法，通过凝固度获取凝固的片段，通过凝固片段的自由扩展获得词的边界，而不需要另外通过边界熵来确定边界，使得长度可以任意长

算法：

1. 统计：统计ngrams频率，计算内部凝固度，删除小于一定阈值的词（不同ngrams设置不同的阈值）
2. 切分：用上述grams对语料进行切分（粗糙的分词），并统计频率。切分的规则是，只要一个片段出现在前一步得到的集合G中，这个片段就不切分，比如“各项目”，只要“各项”和“项目”都在G中，这时候就算“各项目”不在G中，那么“各项目”还是不切分，保留下来；
3. 回溯：检查如果它是一个小于等于n字的词，那么检测它在不在G中，不在就出局；如果它是一个大于n字的词，那个检测它每个n字片段是不是在G中，只要有一个片段不在，就出局。

具体：

ngrams取4， 最小词频5，凝聚度筛选{2: 5, 3: 25, 4: 125}

## 方法2：

smoothnlp

https://github.com/smoothnlp/SmoothNLPgithub.com

利用内部凝聚度和左右信息熵计算得分

算法：

1. 生成候选词：直接将文本按字符分割后拼接为候选词。即把文本按标点切分成句，提取每句话的ngrams
2. 候选词得分计算：计算每个候选词得分，表示成词可能性，度量左右邻字丰富程度使用了左右信息熵差的绝对值构造的统计量L，度量内聚程度使用的平均互信息AMI，得分=L+AMI

具体：

min_n=2, max_n=4，因此最大的词只能是4，最小词频5

### 方法3：

hanlp

跟smoothnlp类似，使用信息熵和互信息

https://github.com/hankcs/pyhanlpgithub.com

## 结果对比：

使用jieba分词对语料进行分词得到词表，将上述两种算法所得词表与jieba分词作比较，看看有哪些词是jieba分词结果没有的，是否为’新词‘[hankcs/pyhanlp](https://github.com/hankcs/pyhanlp)使用jieba分词对语料进行分词得到词表，将上述两种算法所得词表与jieba分词作比较，看看有哪些词是jieba分词结果没有的，是否为’新词‘

结果如下:

![img](https://pic4.zhimg.com/80/v2-199658e799d30e2b3520f192d2a9bbc0_720w.png)

可以看到smoothnlp top50有36氪、机器学习、新浪微博、比特币等，但这些可能是jieba分成了机器、学习，新浪、微博之类的

苏神的算法top50有很多的人名，还有钢铁侠、蚂蜂窝、蜗牛邦、石墨烯、树莓派、抓钱猫、捞月狗、纸牌屋、迷你仓、荔枝FM、垂直导购等新兴词汇，并且通过凝固片段的自由扩展获得词的边界，而不需要另外通过边界熵来确定边界，使得长度可以任意长。

再比较下苏神算法里识别出而smoothnlp和hanlp里没有的词：

![img](https://pic2.zhimg.com/80/v2-6ee6d5b2816537bdda696126a73657cb_720w.png)

上面是苏神算法里有而smoothnlp里没有的，下面是hanlp里没有的

最后看下苏神算法里有，而smoothnlp和hanlp都没有识别出来的词：

![img](https://pic1.zhimg.com/80/v2-572a31b28ef390d7aee8f0b96260e00d_720w.png)

![img](https://pic1.zhimg.com/80/v2-7055fc520797edab790b0747591d4276_720w.png)

可以看到有个很多新词是smoothnlp hanlp没有识别到的，例如很多人名，还有捞月狗、抓财猫、唐驳虎、纸牌屋、迷你仓、KK直播、豆瓣小组、猫眼电影、科大讯飞、曲面屏，普罗大众等

结论

初次使用，苏神的算法结果明显好于smoothnlp、hanlp（也可能我的打开方式不对），不过效率需要优化下